<div align="center">
<h1>Offical Implementation of CityGS-ğ’³</h1>

<a href="https://arxiv.org/abs/2503.23044" target="_blank" rel="noopener noreferrer">
  <img src="https://img.shields.io/badge/Paper-VGGT" alt="Paper PDF">
</a>
<a href="https://arxiv.org/abs/2503.23044"><img src="https://img.shields.io/badge/arXiv-2503.23044-b31b1b" alt="arXiv"></a>
<a href="https://lifuguan.github.io/CityGS-X/"><img src="https://img.shields.io/badge/Project_Page-green" alt="Project Page"></a>


**Northwestern Polytechnical University**; **Shanghai AI Lab**

| CityGS-X : A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction


[Yuanyuan Gao*](https://scholar.google.com/citations?hl=en&user=1zDq0q8AAAAJ), [Hao Li*](https://lifuguan.github.io/), [Jiaqi Chen*](https://github.com/chenttt2001), Zhengyu Zou, [Zhihang Zhongâ€ ](https://zzh-tech.github.io), [Dingwen Zhangâ€ ](https://vision-intelligence.com.cn), [Xiao Sun](https://jimmysuen.github.io), Junwei Han<br>(\* indicates equal contribution, â€  means Co-corresponding author)<br>

</div>

![Teaser image](assets/cityx_tease.jpg)

## Todo List
- [ ] Release the training & inference code.
- [ ] Release all model checkpoints.


## Installation

We tested on a server configured with Ubuntu 18.04, cuda 11.6 and gcc 9.4.0. Other similar configurations should also work, but we have not verified each one individually.

1. Clone this repo:

```
git clone https://github.com/gyy456/CityGS-X.git --recursive
cd CityGS-X
```

2. Install dependencies

```
SET DISTUTILS_USE_SDK=1 # Windows only
conda env create --file environment.yml
conda activate citygx-x
```

### Depth regularization


When training on a synthetic dataset, depth maps can be produced and they do not require further processing to be used in our method.

For real world datasets depth maps should be generated for each input images, to generate them please do the following:
1. Clone [Depth Anything v2](https://github.com/DepthAnything/Depth-Anything-V2?tab=readme-ov-file#usage):
    ```
    git clone https://github.com/DepthAnything/Depth-Anything-V2.git
    ```
2. Download weights from [Depth-Anything-V2-Large](https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true) and place it under `Depth-Anything-V2/checkpoints/`
3. Generate depth maps:
   ```
   python Depth-Anything-V2/run.py --encoder vitl --pred-only --grayscale --img-path <path to input images> --outdir <output path>
   ```
5. Generate a `depth_params.json` file using (set the depth image reslution align with the training reslution you want):
    ```
    python utils/make_depth_scale.py --base_dir <path to colmap> --depths_dir <path to generated depths>
    ```
6. use the multi-view constrains to filter the depth:
    ```
    python  multi_view_precess.py  -s  datasets/<scene_name> --resolution 4    --model_path datasets/<scene_name>/train/mask  --images train/rgbs  --pixel_thred 1
    ```

- pixel_thred: filter the pixel_loss > thred;
## Data

First, create a ```data/``` folder inside the project path by 

```
mkdir data
```

The data structure will be organised as follows:

```
data/
â”œâ”€â”€ scene_name
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ rgbs
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ depths
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ mask
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ rgbs
â”‚   â”‚   â”‚   â”œâ”€â”€ 000000.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ sparse/
â”‚   â”‚   â””â”€â”€0/
...
```




## Evaluation

We've integrated the rendering and metrics calculation process into the training code. So, when completing training, the ```rendering results```, ```fps``` and ```quality metrics``` will be printed automatically. And the rendering results will be save in the log dir. Mind that the ```fps``` is roughly estimated by 

```
torch.cuda.synchronize();t_start=time.time()
rendering...
torch.cuda.synchronize();t_end=time.time()
```

which may differ somewhat from the original 3D-GS, but it does not affect the analysis.

Meanwhile, we keep the manual rendering function with a similar usage of the counterpart in [3D-GS](https://github.com/graphdeco-inria/gaussian-splatting), one can run it by 

```
python render.py -m <path to trained model> # Generate renderings
python metrics.py -m <path to trained model> # Compute error metrics on renderings
```

## Training

### Training multiple scenes

To train multiple scenes in parallel, we provide batch training scripts: 

 - Mill-19 and UrbanScene3D: ```train_mill.sh```

 - MatrixCity: ```train_matrix_city.sh```

 run them with 

 ```
bash train_xxx.sh
 ```

 > Notice 1: Make sure you have enough GPU cards and memories to run these scenes at the same time.

 > Notice 2: Each process occupies many cpu cores, which may slow down the training process. Set ```torch.set_num_threads(32)``` accordingly in the ```train.py``` to alleviate it.

### Training a single scene on 4 gpu (for example)


```
torchrun --standalone --nnodes=1 --nproc-per-node=<gpu_num>  train.py --bsz <bsz> -s datasets/<scene_name> --resolution 4 --model_path output/<save_path> --iterations 100000 --images train/rgbs \
    --single_view_weight_from_iter 10000  --depth_l1_weight_final 0.01 --depth_l1_weight_init 1 --dpt_loss_from_iter 10000  --multi_view_weight_from_iter 30000 \
    # --not_use_dpt_loss --not_use_multi_view_loss --not_use_single_view_loss 

```

- not_use_dpt_loss: you can jump Step2 for depth supervision;
- not_use_multi_view_loss: you can jump Step3 for depth supervision;
- not_use_single_view_loss: you can choose not use the single-view geometric loss;
- gpu_num: specify the GPU number to use ;
- bsz: set the taining batch size;
- single_view_weight_from_iter: set the start iteration of the single-view geometric loss default 10_000;
- scale_loss_from_iter:  set the start iteration of the scale loss default 0;
- dpt_loss_from_iter: set the start iteration of the depth supervision default 10_000;
- multi_view_weight_from_iter: seet the start iteration of multi-view constrains default 30_000;
- default_voxel_size: set the mean voxel size of the anchor default 0.0001;
- distributed_dataset_storage: if cpu memory is enough set it False (Load all the depth and gray image on every process), if cpu memory is not enough set it Ture(Load depth and gray image on one process and broadcast to other process).
- distributed_save: if Ture load the final model seperately by process, if False load the final model in one model.

## Acknowledgement
We would like to express our gratitude to the authors of the following algorithms and libraries, which have greatly inspired and supported this project:

- [Grendel-GS: On Scaling Up 3D Gaussian Splatting Training](https://github.com/nyu-systems/Grendel-GS)
- [PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction](https://zju3dv.github.io/pgsr)
- [Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians](https://city-super.github.io/octree-gs/)
- [CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes](https://dekuliutesla.github.io/CityGaussianV2)
- [Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction](https://github.com/Jixuan-Fan/Momentum-GS)



Your contributions to the open-source community have been invaluable and are deeply appreciated.

## BibTeX

```bibtex
@misc{gao2025citygsxscalablearchitectureefficient,
      title={CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction}, 
      author={Yuanyuan Gao and Hao Li and Jiaqi Chen and Zhengyu Zou and Zhihang Zhong and Dingwen Zhang and Xiao Sun and Junwei Han},
      year={2025},
      eprint={2503.23044},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.23044}, 
}
```
